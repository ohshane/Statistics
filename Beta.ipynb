{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta distribution\n",
    "\n",
    "[Continuous Distributions: Beta and Dirichlet Distributions (Youtube)](https://youtu.be/CEVELIz4WXM?si=3vzpeibZHCdCTRs0)\\\n",
    "[102C Lesson 1-2 Beta-Binomial model (Lecture 1)(Youtube)](https://youtu.be/TrO4ANmlzDg?si=W3QmGzyTwubc404G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Beta}:\\text{Binomial} = \\text{Dirichlet}:\\text{Multinomial}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Useful distribution for modeling percentages and proportions.\\\n",
    "(Unlike a normal distribution, the Beta distribution (PDF) is over the interval $[0,1]$ which allows a probability like value.)\n",
    "- The density is proportional to $x^{\\alpha-1}(1-x)^{\\beta-1}$, related to Bernoulli distribution: $x^{\\theta}(1-x)^{1-\\theta}$.\\\n",
    "Bernoulli distribution allows discrete $0$ and $1$, Beta distrubution allows continuous values between $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def beta_plot(a=.1, b=.1):\n",
    "    x = np.linspace(0, 1, 101)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "    alphas = [.5, 5, 1, 2, 2]\n",
    "    betas  = [.5, 1, 3, 2, 5]\n",
    "    for _a, _b in zip(alphas, betas):\n",
    "        ax[0].plot(x, beta.pdf(x, a=_a, b=_b), label=rf\"$\\alpha={_a}, \\beta={_b}$\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlim([0,1])\n",
    "    ax[0].set_ylim([0,4])\n",
    "\n",
    "    ax[1].plot(x, beta.pdf(x, a=a, b=b), label=rf\"$\\alpha={a}, \\beta={b}$\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlim([0,1])\n",
    "    ax[1].set_ylim([0,4])\n",
    "    plt.show()\n",
    "\n",
    "interactive(beta_plot, a=(.1, 10, .1), b=(.1, 10, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "f(x;\\alpha, \\beta)\n",
    "&= C \\cdot x^{\\alpha - 1} (1-x)^{\\beta - 1} \\\\\n",
    "&= \\dfrac{x^{\\alpha - 1} (1-x)^{\\beta - 1}}{\\int_0^1 u^{\\alpha - 1} (1-u)^{\\beta - 1} du} \\\\\n",
    "&= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha - 1} (1-x)^{\\beta - 1} \\\\\n",
    "&= \\frac{1}{\\mathrm B(\\alpha, \\beta)} x^{\\alpha - 1} (1-x)^{\\beta - 1} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma(x) = (x-1)!\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $E[X] = \\dfrac{\\alpha}{\\alpha + \\beta}$\n",
    "- $0 \\leq x \\leq 1$\n",
    "- $\\alpha > 0$\n",
    "- $\\beta > 0$\n",
    "\n",
    "Distribution is symmetric when $\\alpha = \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalizing constant $\\mathrm{B}(\\alpha, \\beta)$ is closely related to $n \\choose k$.\\\n",
    "Let's start by deriving a gamma function $\\Gamma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma funciton\n",
    "Gamma function is used to extend the concept of a factorial function.\n",
    "\n",
    "[ONE NEAT PROOF! Deriving the EULER DEFINITION of the Gamma Function! (Youtube)](https://youtu.be/2GqqEXMPMlE?si=JWHwI0RFhO-3ZPkT)\\\n",
    "[102C Lesson 1-2 Beta-Binomial model (Lecture 1) (Youtube)](https://youtu.be/TrO4ANmlzDg?si=TyREbiUvPTK9v48T)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2b/Generalized_factorial_function_more_infos.svg\" height=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euler's definition\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Gamma(n) &= (n-1)! \\quad (n \\in \\N) \\\\\n",
    "\\Gamma(z) &= (z-1)! \\quad (z \\in \\R^+) \\\\\n",
    "% &= \\frac{z!}{z} \\\\\n",
    "% &= \\frac{1 \\cdot 2 \\cdots z}{z} \\\\\n",
    "&= \\frac{\\overbrace{1 \\cdot 2 \\cdots (z-1) z}^{z} \\overbrace{(z+1) (z+2) \\cdots (z+n)}^{n}}{z (z+1) (z+2) \\cdots (z+n)} \\qquad \\text{where,}\\ z \\ll n \\\\\n",
    "&= \\frac{\\overbrace{1 \\cdot 2 \\cdots z \\cdots (n-1) n}^{n} \\overbrace{(n+1)(n+2) \\cdots (n+z)}^{z}}{z (z+1) (z+2) \\cdots (z+n)} \\\\\n",
    "&= \\frac{n!}{z} \\cdot \\frac{\\overbrace{(n+1)(n+2) \\cdots (n+z)}^{z}}{\\underbrace{(z+1) (z+2) \\cdots (z+n)}_{n}} \\\\\n",
    "&= \\frac{n!}{z} \\cdot \\frac{n^z \\left( 1+\\dfrac{1}{n} \\right) \\left( 1+\\dfrac{2}{n} \\right) \\cdots \\left( 1+\\dfrac{z}{n} \\right)}{n! \\left( \\dfrac{z+1}{1} \\right) \\left( \\dfrac{z+2}{2} \\right) \\cdots \\left( \\dfrac{z+n}{n} \\right)} \\\\\n",
    "&= \\frac{1}{z} \\cdot \\frac{n^z \\left( 1+\\dfrac{1}{n} \\right) \\left( 1+\\dfrac{2}{n} \\right) \\cdots \\left( 1+\\dfrac{z}{n} \\right)}{\\left( \\dfrac{z+1}{1} \\right) \\left( \\dfrac{z+2}{2} \\right) \\cdots \\left( \\dfrac{z+n}{n} \\right)} \\quad \\text{where,}\\ n! \\neq 0 \\ \\\\\n",
    "\\\\\n",
    "\n",
    "\\lim_{n \\to \\infty} n\n",
    "&= \\lim_{n \\to \\infty} \\left( \\frac{2}{1} \\cdot \\frac{3}{2} \\cdot \\frac{4}{3} \\cdots \\frac{n+1}{n} \\right) \\\\\n",
    "&= \\prod_{n=1}^{\\infty} \\left( \\frac{n+1}{n} \\right) \\\\\n",
    "&= \\prod_{n=1}^{\\infty} \\left( 1 + \\frac{1}{n} \\right) \\\\\n",
    "\n",
    "\\lim_{n \\to \\infty} n^z\n",
    "&= \\prod_{n=1}^{\\infty} \\left( 1 + \\frac{1}{n} \\right)^{z} \\\\\n",
    "\\\\\n",
    "\\lim_{n \\to \\infty} \\left( 1+\\dfrac{1}{n} \\right) \\left( 1+\\dfrac{2}{n} \\right) \\cdots \\left( 1+\\dfrac{z}{n} \\right)\n",
    "&= 1 \\\\\n",
    "\\\\\n",
    "\n",
    "\\lim_{n \\to \\infty} \\left( \\dfrac{z+1}{1} \\right) \\left( \\dfrac{z+2}{2} \\right) \\cdots \\left( \\dfrac{z+n}{n} \\right)\n",
    "&= \\lim_{n \\to \\infty} \\left( 1+\\dfrac{z}{1} \\right) \\left( 1+\\dfrac{z}{2} \\right) \\cdots \\left( 1+\\dfrac{z}{n} \\right) \\\\\n",
    "&= \\prod_{n=1}^{\\infty} \\left( 1 + \\frac{z}{n} \\right) \\\\\n",
    "\\\\\n",
    "\n",
    "\\therefore \\Gamma(z) &= \\frac{1}{z} \\prod_{n=1}^{\\infty} \\frac{\\left( 1 + \\dfrac{1}{n} \\right)^z}{1 + \\dfrac{z}{n}} = \\int_{0}^{\\infty} t^{z-1} e^{-t} dt\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} = {\\alpha + \\beta - 2 \\choose \\alpha - 1} \\cdot (\\alpha + \\beta - 1) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x; \\alpha, \\beta)\n",
    "&= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha - 1} (1-x)^{\\beta - 1} \\\\\n",
    "\n",
    "f(x; \\alpha+1, \\beta+1)\n",
    "&= \\frac{\\Gamma(\\alpha + \\beta + 2)}{\\Gamma(\\alpha+1) \\Gamma(\\beta+1)} x^{\\alpha} (1-x)^{\\beta} \\\\\n",
    "&= (\\alpha + \\beta + 1) \\frac{(\\alpha + \\beta)!}{\\alpha!\\ \\beta!} x^{\\alpha} (1-x)^{\\beta} \\\\\n",
    "&= (\\alpha + \\beta + 1) {\\alpha + \\beta \\choose \\alpha} x^{\\alpha} (1-x)^{\\beta}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli + Binomial + Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom, beta\n",
    "from ipywidgets import interactive\n",
    "import math\n",
    "\n",
    "def plot(n=10, p=0.6):\n",
    "    k = round(n*p)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(24, 6))\n",
    "    ax[0].set_title(rf'$Bernoulli(x;p={p})$')\n",
    "    X = np.arange(0,2)\n",
    "    ax[0].stem(X,[1-p,p])\n",
    "    ax[0].set_ylim(0,1)\n",
    "    ax[0].set_xticks(X)\n",
    "    ax[0].set_xlabel(r'$X$')\n",
    "\n",
    "    ax[1].set_title(rf'$Binomial(x;n={n},p={p})$')\n",
    "    X = np.arange(0,n+1)\n",
    "    ax[1].stem(X, binom.pmf(X,n,p))\n",
    "    ax[1].set_ylim(0)\n",
    "    ax[1].set_xticks(X)\n",
    "    ax[1].set_xlabel(r'$X$')\n",
    "\n",
    "    ax[2].set_title(rf'$Beta(x;\\alpha={k+1},\\beta={n-k+1})$')\n",
    "    X = np.linspace(0,1,201)\n",
    "    ax[2].plot(X, beta.pdf(X,a=k+1,b=n-k+1))\n",
    "    ax[2].stem(k/n, beta.pdf(k/n,a=k+1,b=n-k+1), linefmt=':', markerfmt='')\n",
    "    ax[2].set_xlim(0,1)\n",
    "    ax[2].set_ylim(0)\n",
    "    ax[2].set_xlabel(r'$X$')\n",
    "\n",
    "    X = np.linspace(0,1,201)\n",
    "    ax[3].plot(X, beta.pdf(X,a=k,b=n-k+1), label=rf\"$Beta(x;\\alpha={k},\\beta={n-k+1})$\", ls=':')\n",
    "    ax[3].plot(X, beta.pdf(X,a=k+1,b=n-k+1), label=rf\"$Beta(x;\\alpha={k+1},\\beta={n-k+1})$\")\n",
    "    ax[3].plot(X, beta.pdf(X,a=k+1,b=n-k), label=rf\"$Beta(x;\\alpha={k+1},\\beta={n-k})$\", ls=':')\n",
    "    alpha = 0.05\n",
    "    lo = beta.ppf(  alpha/2, k, n-k+1)\n",
    "    hi = beta.ppf(1-alpha/2, k+1, n-k)\n",
    "    lo = 0 if math.isnan(lo) else lo\n",
    "    hi = 1 if math.isnan(hi) else hi\n",
    "    if lo != 0:\n",
    "        ax[3].stem(lo, beta.pdf(lo,a=k,b=n-k+1), linefmt=':')\n",
    "    if hi != 1:\n",
    "        ax[3].stem(hi, beta.pdf(hi,a=k+1,b=n-k), linefmt=':')\n",
    "    ax[3].set_title(f'Clopper-Pearson interval [{lo:.3f}-{hi:.3f}]')\n",
    "    ax[3].set_xlim(0,1)\n",
    "    ax[3].set_ylim(0)\n",
    "    ax[3].legend()\n",
    "    ax[3].set_xlabel(r'$X$')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interactive(plot, n=(1,40,1), p=(0,1,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' Rule\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr}(\\theta|\\mathbf x) = \\frac{\\mathrm{Pr}(\\mathbf x|\\theta)\\mathrm{Pr}(\\theta)}{\\mathrm{Pr}(\\mathbf x)}\n",
    "$$\n",
    "\n",
    "- Posterior: $\\mathrm{Pr}(\\theta|\\mathbf x)$ is the probability distribution that we want to find.\n",
    "- Likelihood: $\\mathrm{Pr}(\\mathbf x|\\theta)$ is the probability of observed data for a given value of $\\theta$.\n",
    "- Prior: $\\mathrm{Pr}(\\theta)$ is prior probability distribution of $\\theta$.\n",
    "- Marginal probability: $\\mathrm{Pr}(\\mathbf x)$ is the probability of observing the values in our data regardless of the value $\\theta$. This is equal to the intergral of the numerator across all possible values of $\\theta$. This is a constant.\n",
    "\n",
    "Because the marginal probability $\\mathrm{Pr}(\\mathbf x)$ is a constant, the posterior distribution of $\\theta$ is proportional to the likelihood function times the prior distribution.\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr}(\\theta|\\mathbf x) \\propto \\mathrm{Pr}(\\mathbf x|\\theta)\\mathrm{Pr}(\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [1, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1]]),\n",
       " array([1, 1, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "words = [\"meeing\", \"due\", \"thanks\", \"hello\", \"cool\"]\n",
    "df = pd.DataFrame(data={\n",
    "    \"cool\"      : [1,1,0,1,1,0],\n",
    "    \"hello\"     : [1,0,1,1,0,0],\n",
    "    \"thanks\"    : [0,0,1,1,1,0],\n",
    "    \"due\"       : [0,0,1,1,1,1],\n",
    "    \"meeting\"   : [0,0,0,0,1,1],\n",
    "    \"spam label\": [1,1,1,0,0,0],\n",
    "})\n",
    "\n",
    "X = df.iloc[:,:-1].to_numpy()\n",
    "y = df.iloc[:, -1].to_numpy()\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69508143])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def BCELoss(y_pred, y_true):\n",
    "    return -(y_true @ np.log(y_pred).T + (1-y_true) @ np.log(1-y_pred).T) / len(y_true)\n",
    "\n",
    "w = np.random.randn(1,5) * 0.01\n",
    "b = np.zeros((1,1))\n",
    "\n",
    "z = w@X.T + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "BCELoss(a, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3766])\n",
      "tensor([1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.softmax(0))\n",
    "\n",
    "x.exp() / x.exp().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "4 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5, 0.6666666666666666)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "# One-hot encoding for spam and ham emails with a small vocabulary\n",
    "# Let's assume a vocabulary of 5 words: \"free\", \"win\", \"offer\", \"hello\", \"click\"\n",
    "# A one-hot vector [1, 0, 0, 0, 1] would mean the message contains \"free\" and \"click\"\n",
    "\n",
    "# Sample one-hot encoded messages\n",
    "spam_emails = np.array([\n",
    "    [1, 1, 0, 0, 0],  # Spam message containing \"free\" and \"win\"\n",
    "    [1, 0, 1, 0, 0],  # Spam message containing \"free\" and \"offer\"\n",
    "    [0, 1, 1, 0, 0]   # Spam message containing \"win\" and \"offer\"\n",
    "])\n",
    "\n",
    "ham_emails = np.array([\n",
    "    [0, 0, 0, 1, 1],  # Ham message containing \"hello\" and \"click\"\n",
    "    [0, 0, 0, 1, 0],  # Ham message containing \"hello\"\n",
    "    [0, 0, 0, 0, 1]   # Ham message containing \"click\"\n",
    "])\n",
    "\n",
    "# We will perform Bayesian updates on the word \"free\"\n",
    "# Let's define some priors based on our belief before seeing the data\n",
    "# Prior belief: probability that any email containing \"free\" is spam\n",
    "prior_alpha = 2  # Prior number of successes (alpha) - somewhat believe it's spam\n",
    "prior_beta = 2   # Prior number of failures (beta) - somewhat believe it's not spam\n",
    "\n",
    "# Count the number of spam emails containing the word \"free\"\n",
    "spam_word_count = np.sum(spam_emails[:, 0])  # Count in the first column for \"free\"\n",
    "print(spam_word_count)\n",
    "\n",
    "# Total emails containing the word \"free\"\n",
    "total_emails_with_word = np.sum(spam_emails[:, 0]) + np.sum(ham_emails[:, 0])\n",
    "print(total_emails_with_word)\n",
    "\n",
    "# Perform Bayesian update with the counts\n",
    "posterior_alpha = prior_alpha + spam_word_count\n",
    "posterior_beta = prior_beta + (total_emails_with_word - spam_word_count)\n",
    "print(posterior_alpha, posterior_beta)\n",
    "\n",
    "# Create a beta distribution with the updated alpha and beta\n",
    "posterior_distribution = beta(posterior_alpha, posterior_beta)\n",
    "\n",
    "# Now let's print out our prior and posterior beliefs\n",
    "prior_distribution = beta(prior_alpha, prior_beta)\n",
    "\n",
    "prior_mean = prior_distribution.mean()\n",
    "posterior_mean = posterior_distribution.mean()\n",
    "\n",
    "prior_mean, posterior_mean\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
